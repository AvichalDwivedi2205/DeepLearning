{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X1A3uD2-CQ_t"
      },
      "outputs": [],
      "source": [
        "# In this one we will see the implementation of different actibation functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step Function\n",
        "# This will be zero and one and nothing in between\n",
        "def step_function(x):\n",
        "    if x < 0:\n",
        "        return 0\n",
        "    return 1"
      ],
      "metadata": {
        "id": "NZNfQ06PId8e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(step_function(-1))\n",
        "print(step_function(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOeEzLo0JfOi",
        "outputId": "b744361c-032e-4be9-ba67-ed56f7b5178a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid Function\n",
        "# This is better than sigmoid if something depends o n more than one factor as it will give a value close to 1 or 0 generally\n",
        "# So we can differentiate that if a value is very close to 1 then we can say it as true or else as false\n",
        "import math\n",
        "def sigmoid_function(x):\n",
        "    return 1 / (1 + math.exp(-x))"
      ],
      "metadata": {
        "id": "B-mMI-fGJIol"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sigmoid_function(1))\n",
        "print(sigmoid_function(100))\n",
        "print(sigmoid_function(-17))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT2T2cFeJdI_",
        "outputId": "e9ecd9e9-8802-45d8-ccfa-b55ff8f077f1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7310585786300049\n",
            "1.0\n",
            "4.1399375473943306e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tanh Function\n",
        "# This function is similar to sigmoid function with the only difference being that in this case we have values from -1 to 1 instead of 0 to 1\n",
        "# This can also be very well used for differentiating between true and false as well\n",
        "def tanh_function(x):\n",
        "  return (math.exp(x) - math.exp(-x))/(math.exp(x) + math.exp(-x))"
      ],
      "metadata": {
        "id": "RgYYYn0dJzVd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tanh_function(1))\n",
        "print(tanh_function(100))\n",
        "print(tanh_function(-17))\n",
        "# Use Sigmoid In Output Layers. But in all the other places try to use tanh.\n",
        "# This is because it will calculate a mean of zero and will center the data\n",
        "\n",
        "# Problem With Sigmoid and Tanh\n",
        "# The derivative of tanh and sigmoid tends to zero so it is bad for backpropogation as we need to find the derivative and backpropogate for errors\n",
        "# This is called the vanishing gradient problem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0MtG-1lKVIO",
        "outputId": "0869c781-cb4b-4d20-c3c0-23e2eac1300d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7615941559557649\n",
            "1.0\n",
            "-0.9999999999999966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Relu Function\n",
        "# This is a really simple function which is zero below zero and x after zero\n",
        "# essentially max(0,x)\n",
        "def relu_function(x):\n",
        "  return max(0,x)\n",
        "# This is the most popular used deep learning function especially in hidden layers\n",
        "# If you are unsure what to use in hidden layers go for RELU as your default choice"
      ],
      "metadata": {
        "id": "8I-jxnM1KgTc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(relu_function(-5))\n",
        "print(relu_function(5))\n",
        "print(relu_function(50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf0ZqpjxLrSd",
        "outputId": "b7255ec6-c477-4185-84bc-642c2ae11ddb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "5\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leaky Relu\n",
        "# This is similar to relu and is trying to reduce value below zero to zero but not completelt does it it is 0.1x below zero and x after that\n",
        "def leakyrelu_function(x):\n",
        "  return max(0.1*x,x)"
      ],
      "metadata": {
        "id": "N_5343P8LzWs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(leakyrelu_function(-5))\n",
        "print(leakyrelu_function(5))\n",
        "print(leakyrelu_function(50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUTjePlyMZvK",
        "outputId": "b442a6fa-3575-4264-b9de-e33c8e33de7d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.5\n",
            "5\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If binary classification in the o/p layer most probably we will use sigmoid\n",
        "# Mostly in hidden layers relu and leaky relu\n",
        "# Therefore it is mostly about experimentation about which hidden and output layer combination works the best"
      ],
      "metadata": {
        "id": "7SKjmU-EMgt_"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}